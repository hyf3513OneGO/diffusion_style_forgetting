{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:29:43.397412Z",
     "start_time": "2024-04-02T01:29:41.373208Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd341963267c464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:03.358308Z",
     "start_time": "2024-04-02T01:29:43.892914Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/yifei/anaconda3/envs/nlp/lib/python3.10/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>.load_config(...) followed by <class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56493069c3904da38113d67bcef9a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "device = torch.device(\"cuda:7\")\n",
    "base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "repo = \"ByteDance/SDXL-Lightning\"\n",
    "ckpt = \"sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n",
    "\n",
    "# Load model.\n",
    "unet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(device, torch.float16)\n",
    "unet.load_state_dict(load_file(hf_hub_download(repo, ckpt)))\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "\n",
    "# Ensure sampler uses \"trailing\" timesteps.\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e59e8c63da4011",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "num_prompts = 2\n",
    "prompt = [\"draw an orange cat in vangogh style\" for i in range(num_prompts)]\n",
    "for i in range(num_samples//num_prompts):\n",
    "    images = pipe(prompt, num_inference_steps=2, guidance_scale=0).images\n",
    "    for idx,image in enumerate(images):\n",
    "        image.save(f\"./datasets/generated/cats/{i*num_prompts+idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382c841618505f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:03.363449Z",
     "start_time": "2024-04-02T01:30:03.359851Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "image_shape = 1024\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((image_shape,image_shape)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 常用标准化\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ca6e2e47ce80d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:04.744482Z",
     "start_time": "2024-04-02T01:30:03.364396Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_net = torchvision.models.vgg16()\n",
    "vgg_encoder = nn.Sequential(*list(vgg_net.features)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707251e838509654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:04.749858Z",
     "start_time": "2024-04-02T01:30:04.746005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_layers, content_layers = [0, 5, 10, 19, 28], [25]\n",
    "\n",
    "def build_style_hooker():\n",
    "    style_features=[]\n",
    "    def style_hooker(module, fea_in, fea_out):\n",
    "        style_features.append(fea_out)\n",
    "    return style_hooker,style_features\n",
    "def build_content_hooker():\n",
    "    content_features=[]\n",
    "    def content_hooker(module,fea_in,fea_out):\n",
    "        content_features.append(fea_out)\n",
    "    return content_hooker,content_features\n",
    "\n",
    "def hook_model(layers,hook,model):\n",
    "    for i,layer in model.named_children():\n",
    "        if int(i) in layers:\n",
    "            layer.register_forward_hook(hook)\n",
    "# style_hooker,style_features = build_style_hooker()\n",
    "# hook_model(style_layers,style_hooker,vgg_encoder)\n",
    "# img = Image.open(\"./bedroom.jpg\")\n",
    "# img_tensor = preprocess(img).to(device)\n",
    "# print(img_tensor.shape)\n",
    "# a=vgg_encoder(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba191be2a1e3f6a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:31:14.057030Z",
     "start_time": "2024-04-02T01:31:14.050722Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gram(x):\n",
    "    #x [batch_size,channels,h,w]\n",
    "    x = x.reshape(x.shape[0],x.shape[1],-1) #[batch_size,channels,h*w]\n",
    "    gram = x@x.transpose(1,2) #[batch_size,channels,channels]\n",
    "    return gram/(x.shape[2])\n",
    "def style_distance(x1,x2):\n",
    "    #return [batch_size]\n",
    "    gram_diff = (compute_gram(x1)-compute_gram(x2))**2\n",
    "    return torch.mean(gram_diff,dim=(-1,-2))\n",
    "# print(style_distance(style_features[0],style_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fdd6e069cc68d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:19.535926Z",
     "start_time": "2024-04-02T01:30:19.532086Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def frozen_all(model):\n",
    "    for parameters in model.parameters():\n",
    "        parameters.requires_grad = False\n",
    "def activate_all(model):\n",
    "    for parameters in model.parameters():\n",
    "        parameters.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b0f7b34500f89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:21.803588Z",
     "start_time": "2024-04-02T01:30:21.799270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae = pipe.vae\n",
    "unet = pipe.unet\n",
    "clip1 = pipe.text_encoder\n",
    "clip2 = pipe.text_encoder_2\n",
    "model_components = {\"vae\":vae,\"unet\":unet,\"clip1\":clip1,\"clip2\":clip2,\"vgg_encoder\":vgg_encoder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53048d81e230f600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:25.708475Z",
     "start_time": "2024-04-02T01:30:25.703478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forget_prompts_num=10\n",
    "forget_prompts = []\n",
    "neutral_prompts_num=10\n",
    "neutral_prompts = [\n",
    "    \"An ancient library hidden in a lush, mystical forest, bathed in golden sunlight filtering through the trees\",\n",
    "    \"A futuristic cityscape at night, illuminated by neon lights with flying cars zooming between skyscrapers\",\n",
    "    \"A serene mountain landscape in the early morning, with low-hanging clouds and the first light of dawn painting the peaks in warm colors\",\n",
    "    \"A bustling medieval market square during a festival, with colorful stalls, flags waving, and lively crowds\",\n",
    "    \"An underwater city with bioluminescent plants and creatures, ancient ruins, and bubble-like homes\",\n",
    "    \"A steampunk laboratory filled with intricate machinery, glowing tubes, gears, and an inventor working on a new creation\",\n",
    "    \"A vast desert with a nomadic caravan at sunset, silhouetted against the vibrant colors of the sky, with distant mountains\",\n",
    "    \"An enchanted forest in autumn, with a path winding through trees with multicolored leaves, and a hint of magic in the air\",\n",
    "    \"A space station orbiting a distant planet, with astronauts looking out at the stars from a large viewing dome\",\n",
    "    \"A post-apocalyptic city being reclaimed by nature, with skyscrapers covered in vines and wildlife wandering the streets\"\n",
    "]\n",
    "for i in range(forget_prompts_num):\n",
    "    color = random.choice(['red','black','blue','white'])\n",
    "    prompt = f\"draw a {color} cat with vangogh style\" #remember to change for robust test\n",
    "    forget_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d533f2320a0d5bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:30:29.072644Z",
     "start_time": "2024-04-02T01:30:28.617315Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "forget_tensors = []\n",
    "forget_dataset_dir = 'datasets/generated/cats'\n",
    "for file in os.listdir(forget_dataset_dir):\n",
    "    image = Image.open(os.path.join(forget_dataset_dir,file))\n",
    "    forget_tensors.append(preprocess(image))\n",
    "forget_tensors=torch.stack(forget_tensors,dim=0).to(device)\n",
    "print(forget_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a4547d1e9c3bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T15:55:18.288073Z",
     "start_time": "2024-04-01T15:55:18.269353Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_tensors[1,:].repeat(2,1,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a50299b-a8a9-41e3-9c7c-40688c5dbaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:39:37.706502Z",
     "start_time": "2024-04-02T01:39:37.683841Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 2\n",
    "vgg_encoder.eval()\n",
    "style_hooker,style_features = build_style_hooker()\n",
    "hook_model(style_layers,style_hooker,vgg_encoder)\n",
    "task_name =\"naive_forget\"\n",
    "save_dir = f'model_weights/{task_name}/{time.strftime(\"%Y%m%d\")}'\n",
    "\n",
    "# unet.load_state_dict(torch.load(\"model_weights/naive_forget/20240402/naive_forget_156959506.5.pt\"))\n",
    "\n",
    "for k,v in model_components.items():\n",
    "    frozen_all(v)\n",
    "activate_all(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf9db6ecfab36cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T01:41:36.915742Z",
     "start_time": "2024-04-02T01:41:34.020934Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a0b651152b47cb9f77a8037d78f271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/100 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m     loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_final\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss_forget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# loss_final.backward()#remmeber to alter\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(unet.parameters())\n",
    "unet.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for batch in range(forget_tensors.shape[0]):\n",
    "        optim.zero_grad()\n",
    "        style_features.clear()\n",
    "        prompt = random.choices(forget_prompts,k=batch_size) #remember to alter\n",
    "        sample_images =  pipe(prompt, num_inference_steps=4, guidance_scale=0).images\n",
    "        sample_tensors = []\n",
    "        for image in sample_images:\n",
    "            sample_tensors.append(preprocess(image))\n",
    "        sample_tensors = torch.stack(sample_tensors,dim=0).to(device)\n",
    "        selected_forget_tensors = forget_tensors[batch,:].repeat(batch_size,1,1,1)\n",
    "        surrogate_tensor = torch.randn_like(sample_tensors)\n",
    "        in_tensors = torch.cat([sample_tensors,selected_forget_tensors],dim=0)\n",
    "        vgg_encoder(in_tensors)\n",
    "        # gram distance for every sample between the forget pics and sampled pics\n",
    "        # print(style_features)\n",
    "        gram_distance = style_distance(style_features[0][:batch_size,:],style_features[0][batch_size:,:])\n",
    "        # print(gram_distance.shape)\n",
    "        #mse for every sample [batch_size]\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        loss_forget = torch.sum((sample_tensors-surrogate_tensor)**2,dim=(-1,-2,-3))\n",
    "        loss_final = torch.sum(torch.exp(-gram_distance)*loss_forget)\n",
    "        # loss_forget.requires_grad=True\n",
    "        # loss_final.requires_grad = True\n",
    "        # loss_forget.requires_grad = True\n",
    "        with torch.no_grad():\n",
    "            loss_epoch += loss_final.item()\n",
    "        loss_forget.backward()\n",
    "        \n",
    "        # loss_final.backward()#remmeber to alter\n",
    "        optim.step()\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        image = pipe(random.choice(forget_prompts), num_inference_steps=4, guidance_scale=0).images[0]\n",
    "        image.save(f\"outputs/{epoch}.jpg\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        torch.save(unet.state_dict(),os.path.join(save_dir,f\"{task_name}_{round(loss_epoch,3)}.pt\"))\n",
    "    print(f\"epoch:{epoch},loss:{loss_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
