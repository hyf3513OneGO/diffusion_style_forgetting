{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed5b2e7-a69e-428d-ba01-cb32e98d8916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:31.932706Z",
     "start_time": "2024-04-02T13:37:29.751798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch.cuda.amp import autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8e23b7-4558-47c8-bd13-25e28acfb1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:35.781778Z",
     "start_time": "2024-04-02T13:37:31.934408Z"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "device = torch.device(\"cuda:6\")\n",
    "model_name = \"stabilityai/sd-turbo\"\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, EulerDiscreteScheduler\n",
    "vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\", use_safetensors=True).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_name, subfolder=\"text_encoder\", use_safetensors=True).to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_name, subfolder=\"unet\", use_safetensors=True\n",
    ").to(device)\n",
    "scheduler =EulerDiscreteScheduler.from_pretrained(model_name,subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa9e3bb3d0a803f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:14:32.008165Z",
     "start_time": "2024-04-02T10:14:32.004144Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EulerDiscreteScheduler {\n",
       "  \"_class_name\": \"EulerDiscreteScheduler\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"beta_end\": 0.012,\n",
       "  \"beta_schedule\": \"scaled_linear\",\n",
       "  \"beta_start\": 0.00085,\n",
       "  \"clip_sample\": false,\n",
       "  \"interpolation_type\": \"linear\",\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"set_alpha_to_one\": false,\n",
       "  \"sigma_max\": null,\n",
       "  \"sigma_min\": null,\n",
       "  \"skip_prk_steps\": true,\n",
       "  \"steps_offset\": 1,\n",
       "  \"timestep_spacing\": \"trailing\",\n",
       "  \"timestep_type\": \"discrete\",\n",
       "  \"trained_betas\": null,\n",
       "  \"use_karras_sigmas\": false\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827dfa3b-75cd-452a-9feb-5b0cbc1ec2fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:35.790935Z",
     "start_time": "2024-04-02T13:37:35.784748Z"
    }
   },
   "outputs": [],
   "source": [
    "fim_prompts = [\n",
    "    \"Serene alpine meadow at dawn\",\n",
    "    \"Cyberpunk cityscape at twilight\",\n",
    "    \"Medieval castle overlooking a village\",\n",
    "    \"Mystical forest with glowing mushrooms\",\n",
    "    \"Futuristic space station in orbit\",\n",
    "    \"Underwater coral city teeming with marine life\",\n",
    "    \"Steampunk airship above a Victorian skyline\",\n",
    "    \"Desert oasis with palm trees and clear skies\",\n",
    "    \"Ancient pyramids beneath a starry night\",\n",
    "    \"Lunar base on the moon's surface\",\n",
    "    \"Bustling market in a fantasy realm\",\n",
    "    \"Gothic cathedral at dusk with a storm approaching\",\n",
    "    \"Post-apocalyptic wasteland with ruins\",\n",
    "    \"High-speed train in a mountain tunnel\",\n",
    "    \"Giant robot overlooking a cityscape\",\n",
    "    \"Enchanted garden with floating lanterns\",\n",
    "    \"Abandoned factory overtaken by nature\",\n",
    "    \"Deep sea exploration with bioluminescent creatures\",\n",
    "    \"Island paradise with a hidden waterfall\",\n",
    "    \"Subterranean cave network with vast chambers\",\n",
    "    \"Flying cars over a neon metropolis\",\n",
    "    \"Snowy village with a cozy inn\",\n",
    "    \"Haunted mansion in a dark forest\",\n",
    "    \"Solar powered farm in a desert\",\n",
    "    \"Interdimensional portal opening in a library\",\n",
    "    \"Viking longship sailing through a stormy sea\",\n",
    "    \"Alien planet with unique flora and fauna\",\n",
    "    \"Colony on Mars with domed habitats\",\n",
    "    \"Art deco skyscrapers with flying buttresses\",\n",
    "    \"Sunken pirate ship in crystal clear water\",\n",
    "    \"Harbor at sunset with seagulls and sailboats\",\n",
    "    \"Zen rock garden in a tranquil temple\",\n",
    "    \"Giant's causeway on an alien world\",\n",
    "    \"Festival of lights in a bustling city\",\n",
    "    \"Dinosaurs roaming a prehistoric landscape\",\n",
    "    \"Flying buttress bridge over a chasm\",\n",
    "    \"Orbiting satellite with Earth in the background\",\n",
    "    \"Cybernetic enhancements in a futuristic surgery\",\n",
    "    \"Crystal clear lake in a mountain valley\",\n",
    "    \"Glacial ice cave with refracted light\",\n",
    "    \"Aerial view of a sprawling desert city\",\n",
    "    \"Underwater research lab with marine visitors\",\n",
    "    \"Mammoth herd on the tundra under a aurora\",\n",
    "    \"Rustic windmill farm against a sunset\",\n",
    "    \"Space elevator with cargo ascending\",\n",
    "    \"Volcanic eruption with lava flowing down the slopes\",\n",
    "    \"Time-lapse of stars moving in the night sky\",\n",
    "    \"Ornate gothic library filled with ancient tomes\",\n",
    "    \"Sailing through a bioluminescent bay\",\n",
    "    \"Dune buggy race across a sandy plain\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbe3129-b7ca-4c1c-aa97-3869c06e25de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:35.800149Z",
     "start_time": "2024-04-02T13:37:35.793664Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_img(prompts,gen_shape=(512,512),num_inference_steps=4,guidance_scale=0.75,requires_grad=True):\n",
    "    text_input = tokenizer(\n",
    "        prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    batch_size = len(prompts)\n",
    "    height,width=gen_shape \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    latents = torch.randn(\n",
    "        (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "        device=device\n",
    "    )\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    for t in scheduler.timesteps:\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "    \n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "    \n",
    "        # predict the noise residual\n",
    "        if requires_grad:\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "    \n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    \n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    if requires_grad:\n",
    "        image = vae.decode(latents).sample\n",
    "        return image\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            image = vae.decode(latents).sample\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e273f9-913c-4a68-8e2b-b4e943b90d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:35.804761Z",
     "start_time": "2024-04-02T13:37:35.801589Z"
    }
   },
   "outputs": [],
   "source": [
    "num_prompts=5\n",
    "fim_samples_num = 100\n",
    "image_shape = 512\n",
    "prompts_all = []\n",
    "# for i in tqdm(range(fim_samples_num//num_prompts)):\n",
    "#     prompt_selected = random.choices(fim_prompts,k=num_prompts)\n",
    "#     images = generate_img(prompt_selected,gen_shape=(image_shape,image_shape),num_inference_steps=2,requires_grad=False)\n",
    "#     prompts_all.extend(prompt_selected)\n",
    "#     for idx,image in enumerate(images):\n",
    "#         image = (image / 2 + 0.5).clamp(0, 1)\n",
    "#         image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "#         Image.fromarray(image).save(f\"datasets/generated/sd_fim/{i * num_prompts + idx}.jpg\")\n",
    "# prompts_all_text = \"\\n\".join(prompts_all)\n",
    "# with open(\"datasets/generated/sd_fim/prompts.txt\",\"w\") as f:\n",
    "#     f.write(prompts_all_text)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ec50cc-50aa-4c12-9989-32ed3d00a91e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:35.809747Z",
     "start_time": "2024-04-02T13:37:35.806455Z"
    }
   },
   "outputs": [],
   "source": [
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((image_shape,image_shape)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 常用标准化\n",
    "])\n",
    "postprocess = transforms.Compose([\n",
    "    lambda x:(x/2+0.5).clamp(0,1),\n",
    "    transforms.ToPILImage()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ee8994-fd7b-4333-83cf-1ad20b13e017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:37:36.328759Z",
     "start_time": "2024-04-02T13:37:35.811063Z"
    }
   },
   "outputs": [],
   "source": [
    "fim_dataset =[]\n",
    "fim_dataset_dir = \"datasets/generated/sd_fim\"\n",
    "for file in os.listdir(fim_dataset_dir):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        image = Image.open(os.path.join(fim_dataset_dir,file))\n",
    "        fim_dataset.append(preprocess(image))\n",
    "fim_dataset = torch.stack(fim_dataset)\n",
    "with open(os.path.join(fim_dataset_dir,\"prompts.txt\"),\"r\") as f:\n",
    "    fim_prompts_datasets = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206dee5a99fb8491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:17:12.296666Z",
     "start_time": "2024-04-02T10:17:12.291146Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e83d13-ba5f-47a2-8094-a2842b6c1c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T13:38:52.972796Z",
     "start_time": "2024-04-02T13:37:36.330659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "batch_size = 2\n",
    "save_path = os.path.join(\"model_weights\",\"fim_sd.pt\")\n",
    "def save_fim():\n",
    "    torch.cuda.empty_cache()\n",
    "    fisher_dict = []\n",
    "    params_mle_dict = []    \n",
    "    mle_params = {name:param.clone() for name,param in unet.named_parameters()}\n",
    "    fisher_dict = {name:torch.zeros_like(param.clone()) for name,param in unet.named_parameters()}\n",
    "    with torch.no_grad():\n",
    "        text_input = tokenizer(\n",
    "        fim_prompts_datasets, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    for _ in tqdm(range(fim_samples_num)):\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            sample_idx = random.choices([i for i in range(batch_size)],k=batch_size)\n",
    "            selected_input_ids = text_input.input_ids[sample_idx,:]\n",
    "            text_embeddings = text_encoder(selected_input_ids.to(device))[0]\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "            uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "            text_embeddings = torch.cat([text_embeddings])\n",
    "            selected_samples = fim_dataset[sample_idx,:].to(device)\n",
    "            selected_x = vae.encode(selected_samples).latent_dist.sample()\n",
    "            noise = torch.randn_like(selected_x).to(device)\n",
    "            time_steps = torch.randint(0,scheduler.config.num_train_timesteps,(1,)).to(device)\n",
    "        x_t = scheduler.add_noise(selected_x,noise,time_steps)\n",
    "        pred = unet(x_t,time_steps,text_embeddings).sample\n",
    "        loss = F.mse_loss(pred,noise)\n",
    "        \n",
    "        loss.backward()\n",
    "        for name, param in unet.named_parameters():\n",
    "            if torch.isnan(param.grad.data).any():\n",
    "                print(\"NAN detected\")\n",
    "            fisher_dict[name] += (param.grad.data ** 2) / batch_size\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(fisher_dict, f)\n",
    "save_fim()         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
